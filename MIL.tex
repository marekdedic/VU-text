\chapter{An algebraic formalism for Multi-instance learning}

\VUname{Multi-instance learning} is a class of machine learning algorithms, first described by \cite{dietterich_solving_1997}, that can be used for both \VUname{supervised} (see \cite{amores_multiple_2013}) as well as \VUname{unsupervised} (see \cite{zhang_multi-instance_2009}, \cite{chen_contextual_2012}) learning. While classical machine learning algorithms typically require a sample to be represented as a vector of numbers, multi-instance learning relaxes this requirement by representing a sample as a \VUname{bag} of an arbitrary number of objects with a singular label for the whole bag. \cite{dedic_hierarchicke_2017} describes a novel approach to the formalization of multi-instance learning, a liberal translation of which constitutes the following chapter.
In order to correctly describe the notion of a bag using mathematical notation, a few theoretical definitions are needed.

\section{Multisets}

A multiset is a set allowing repeated elements, or alternatively an unordered tuple. \cite{knuth_art_1968} defines the concept as follows:

\begin{define}
	Let \( \VUset{A} \) be a set and \( m : \VUset{A} \to \VUfield{N} \setminus \left\{ 0 \right\} \). The tuple \( \left( \VUset{A}, m \right) \) is called a \VUname{multiset} over the set \( \VUset{A} \). For an \( a \in \VUset{A} \), the number \( m \left( a \right) \) is called the \VUname{multiplicity} (i. e. number of occurrences) of \( a \). If \( a \in \VUset{A} \), then \( a \) is called an element of \( \left( \VUset{A}, m \right) \) and denoted as
	\[ a \in \left( \VUset{A}, m \right) \]
\end{define}

Obviously, a multiset is a generalization of a set and as such it is usually written as an enumeration of the elements which are repeated as many times as is their multiplicity.

\begin{example}
	The multiset \( \left( \left\{ a, b, c \right\}, \left\{ \left( a, 4 \right), \left( b, 1 \right), \left( c, 6 \right) \right\} \right) \) may be written as \( \left\{ a, a, a, a, b, c, c, c, c, c, c \right\} \).
\end{example}

\begin{remark}
	A set can be seen as a special case of a multiset where each element has a multiplicity of 1.
\end{remark}

\begin{define}
	Let \( \VUset{A} \) be a set. A multiset \( \left( \VUset{B}, m \right) \) is called a \VUname{submultiset} of \( \VUset{A} \) if \( \VUset{B} \subset \VUset{A} \). This is denoted as
	\[ \left( \VUset{B}, m \right) \subset \VUset{A} \]
\end{define}

The previous definition allows for a generalization of the concept of a power set, which is usually denoted as \( \mathcal{P} \left( \VUset{A} \right) \) or \( 2^{\VUset{A}} \).

\begin{define}
	Let \( \VUset{A} \) be a set. A \VUname{power multiset} of \( \VUset{A} \) is the set
	\[ \mathcal{P}^M \left( \VUset{A} \right) = \left\{ \left( \VUset{B}, m \right) \middle| \left( \VUset{B}, m \right) \text{is a multiset} \wedge \left( \VUset{B}, m \right) \subset \VUset{A} \right\} \]
\end{define}

\begin{remark}
	A power set of \( \VUset{A} \) may be seen as a set of all function from \( \VUset{A} \) to \( \left\{ 0, 1 \right\} \), sometimes written as \( \left\{ 0, 1 \right\}^{\VUset{A}} \). \( \left\{ 0, 1 \right\} \) coincides with the von Neumann ordinal 2, therefore the power set is often denoted \( 2^{\VUset{A}} \). Similarly, the power multiset may be seen as a set of all functions from \( \VUset{A} \) to \( \VUfield{N} \) ( where \( 0 \in \VUfield{N} \)). It is therefore possible to use an analogous notation \( \mathcal{P}^M \left( \VUset{A} \right) = \VUfield{N}^\VUset{A} \). This analogy extends to the size of the sets in question where
	\[ \left\lvert 2^{\VUset{A}} \right\rvert = 2^{\left\lvert \VUset{A} \right\rvert} \qquad \text{and} \qquad \left\lvert \VUfield{N}^\VUset{A} \right\rvert = + \infty \]
	for all \( \VUset{A} \neq \emptyset \).
\end{remark}

\begin{define}
	Let \( \VUset{B} = \left( \VUset{A}, m \right) \) be a multiset where \( \VUset{A} \) is finite. The cardinality of the multiset \( \VUset{B} \) is
	\[ \left\lvert \VUset{B} \right\rvert = \sum_{a \in \VUset{A}} m \left( a \right) \]
\end{define}

\section{Multi-instance learning}
Multi-instance learning (see \cite{dietterich_solving_1997}) is an approach to solving problems where there is an input space \( \VUspace{X} \) and an output space \( \VUspace{Y} \) (whose elements are often called labels or classes) and the goal is to find a mapping of input objects to output objects. Unlike classical supervised learning, there is no known solution consisting of pairs of input and output objects. Instead, the input objects are grouped together into so called bags, which are defined as follows:

\begin{define}
	Let \( \VUspace{X} \) be a set of input objects. Then the \VUname{bag space} is a multiset \( \VUspace{B} \) with the properties
	\begin{enumerate}
		\item \( \VUspace{B} \subset \mathcal{P}^M \left( \VUspace{X} \right) \)
		\item \( \left( \forall x \in \VUspace{X} \right) \left( \exists b \in \VUspace{B} \right) \left( x \in b \right) \)
	\end{enumerate}
	Elements of the bag space are called \VUname{bags}.
\end{define}

Multi-instance learning uses a dataset consisting of pairs of bags and labels to learn a mapping between them. \cite{dietterich_solving_1997} provides the following example of a multi-instance problem:

\begin{example}
	Suppose there is a keyed lock on the door to the supply room in an office. Each staff member has a key chain containing several keys. One key on each key chain can open the supply room door. For some staff members, their supply room key opens only the supply room door; while for other staff members, their supply room key may open one or more other doors (e.g., their office door, the mail room door, the conference room door).

	Suppose you are a lock smith and you are attempting to infer the most general required shape that a key must have in order to open the supply room door. If you knew this required shape, you could predict, by examining any key, whether that key could unlock the door. What makes your lock smith job difficult is that the staff members are uncooperative. Instead of showing you which key on their key chains opens the supply room door, they just hand you their entire key chain and ask you to figure it out for yourself! Furthermore, you are not given access to the supply room door, so you canâ€™t try out the individual keys. Instead, you must examine the shapes of all of the keys on the key rings and infer the answer.
\end{example}

In this example, the keys (or, more precisely their description by a feature vector) are the input objects from \( \VUspace{X} \). The key chains are bags. Even though it makes sense to ask for each key whether it unlocks the supply room, this information is not provided. Instead only a key-chain-level information is available. On the other hand, it may be stated that a key chain opens the supply room door if and only if it contains a key which opens the door. This naturally leads to the following definition:

\begin{define}\label{baglabel}
	Let \( \VUspace{X} \) be an input space and \( \VUspace{Y} \) a label space such that
	\[ \left( \forall x \in \VUspace{X} \right) \left( \exists_1 y_x \in \VUspace{Y} \right) \]
	Let \( \VUspace{B} \) be a bag space. Let maximum be well-defined in \( \VUspace{Y} \). Then the label of the bag \( b \in \VUspace{B} \) is
	\[ y_b = \max_{x \in b} \left( y_x \right) \in \VUspace{Y} \]
\end{define}

As will later be shown in sections \ref{bag-space-paradigm} and \ref{embedded-space-paradigm}, this isn't the only interpretation of multi-instance learning. There are problems where the instance-level labels are not only no known, but even not well-defined as the bag isn't seen as a mere collection of objects but as a distinct object of its own.

\section{Approaches to solving multi-instance problems}
In this section, there are described the three major approaches to solving multi-instance problems. The section is sourced primarily from \cite{pevny_using_2017} and \cite{pevny_discriminative_2016}.

\subsection{Instance-space paradigm}
Instance-space paradigm is the original approach proposed by \cite{dietterich_solving_1997}. The existence of labels for all instances is presumed, even though such labels aren't known. The goal of this approach is to find the instance-level labelling \( f : \VUspace{X} \to \VUspace{Y} \) and use it to model the bag-level label analogically to definition \ref{baglabel}, that is
\[ y_b = \max_{x \in \VUspace{X} } \left( f \left( x \right) \right) \]

There are many applications of instance-space paradigm, the most important of which are described next. (Also see \cite{andrews_support_2003} \cite{zhang_multiple_2006})

\VUname{BP-MIP}, proposed by \cite{zhou_neural_2002}, is an application of multi-instance learning to binary classification, i. e. \( \VUspace{Y} = \left\{ -1, +1 \right\} \). A feedforward neural network is used, its output for the \( j \)-th instance of the \( i \)-th bag denoted \( o_{ij} \). An instance-level error function is defined as
\[ E_{ij} = \begin{cases}
		0 &\text{for} \quad y_{b_i} = -1 \wedge o_{ij} < 0.5 \\
		0 &\text{for} \quad y_{b_i} = +1 \wedge o_{ij} \geq 0.5 \\
		\frac{1}{2} \left( o_{ij} - 0.5 \right)^2 &\text{otherwise}
	\end{cases} \]
This instance-level error function is then used to define a bag-level error function as \footnote{The original article erroneously states the upper bound as \( \left\lvert b_j \right\rvert \)}
\[ E_i = \begin{cases}
		\max_{1 \leq j \leq \left\lvert b_i \right\rvert} E_{ij} &\text{for} \quad y_{b_i} = -1 \\
		\min_{1 \leq j \leq \left\lvert b_i \right\rvert} E_{ij} &\text{for} \quad y_{b_i} = +1
	\end{cases} \]
And this is in turn used to define the global loss function
\[ E = \sum_{i = 1}^{\left\lvert \VUspace{B} \right\rvert} E_i \]
With such a well-defined global loss function, the back-propagation algorithm can be used with a slight modification described in the original article.

\VUname{EM-DD}, proposed by \cite{zhang_em-dd:_2002}, combines the \VUname{expectation-maximization} algorithm (see \cite{dempster_maximum_1977}) with the \VUname{diverse density} algorithm (see \cite{maron_framework_1998}). The EM-DD algorithm starts with a target hypothesis \( h \in \VUspace{Y} \), which is then refined with the EM algorithm. In the E-step, the instance thought to be most responsible for the label of each bag is picked. In the M-step, the gradient ascent algorithm is used to find a new hypothesis \( h' \) which maximizes \( \mathrm{DD} \left( h \right) \). \( \mathrm{DD} \left( h \right) \) corresponds to the likelihood that \( h \) is the actual target. These steps are then repeated, refining the hypothesis.

\subsection{Bag-space paradigm}\label{bag-space-paradigm}
The bag-space paradigm is a formalism in which the notion of instance-level labels is abandoned and only bag-level labels are assumed to exist. The definition \ref{baglabel} is no longer used, so some other way of working with bags must be devised. In order to do this, a bag distance function or a bag kernel function is defined, having the form
\[ k : \VUspace{B} \times \VUspace{B} \to \VUfield{R}_0^+ \]
Because the notion of an instance-level label is no longer used, the sought classification function is of the form \( f : \VUspace{B} \to \VUspace{Y} \).

\VUname{Citation-kNN}, proposed by \cite{wang_solving_2000}, defines a modified Hausdorff distance for bags \( b_1, b_2 \in \VUspace{B} \) as
\[ H \left( b_1, b_2 \right) = \min_{x \in b_1} \min_{y \in b_2} \left\lVert x - y \right\rVert \]
The \( k \)-nearest neighbour algorithm (see \cite{dasarathy_nearest_1991}) is modified for this application by adding a system of \VUname{citations} and \VUname{references}. The \( r \)-nearest references are the nearest neighbours from the vanilla kNN algorithm and the \( c \)-nearest citers are defined as
\[ \mathrm{Citers} \left( x, c \right) = \left\{ x_i \middle| \mathrm{Rank} \left( x_i, x \right) \leq c \wedge x_i \in b \right\} \]
For each bag, the \( r \)-nearest references and the \( c \)-nearest citers are found using the modified Hausdorff distance. If among those there are more positive bags then there are negative ones, the bag is labelled as positive. Otherwise the bag is labelled as negative.

For more applications using the bag-space paradigm, see \cite{wang_solving_2000}, \cite{kwok_marginalized_2007}, \cite{gartner_multi-instance_2002}, \cite{haussler_convolution_1999}, \cite{zhou_multi-instance_2009} and \cite{muandet_learning_2012}.

\subsection{Embedded-space paradigm}\label{embedded-space-paradigm}
In the embedded space paradigm, labels are only defined on the level of bags, same as in the bag-space paradigm. In order for these bag labels to be learned, an embedding function of the form \( \phi : \VUspace{B} \to \bar{\VUspace{X}} \) must be defined, where \( \bar{\VUspace{X}} \) is a new input space, which may or may not be identical to \( \VUspace{X} \). Using this function, each bag can be represented by an input object \( \phi \left( b \right) \in \bar{\VUspace{X}} \), which makes it possible to use any off-the-shelf supervised learning algorithm. Amongst the simplest embedding functions are e. g. element-wise minimum, maximum and mean. A more complicated embedding function may for example apply a neural network to each instance of the bag and subsequently pool the instances using one of the aforementioned functions.

\VUname{MILES}, proposed by \cite{chen_miles:_2006}, uses an instance dictionary \( \VUspace{D} \) to define an embedding function representing the degree of similarity between the bag and the dictionary, that is
\[ \phi : \VUspace{B} \to \VUfield{R}^{\left\lvert \VUspace{D} \right\rvert} \]
which is defined element-wise as
\[ \phi_i \left( b \right) = \sum_{x \in b} k \left( x, d_i \right) \quad \text{where} \quad d_i \in \VUspace{D} \]
where
\[ k \left( x, d \right) = \begin{cases}
		e^{- \frac{1}{\sigma^2} \left\lVert x - d \right\rVert^2} &\text{if } d \text{ is the nearest neighbour of } x \text{ in } \VUspace{D} \\
		0 &\text{otherwise}
	\end{cases} \]
The dictionary is populated by a 1-class support vector machine (see \cite{zhu_1-norm_2004}), which, however, has a high computational complexity.

For more applications using the embedded-space paradigm, see \cite{cheplygina_multiple_2015}, \cite{chen_image_2004} \cite{foulds_learning_2008}, \cite{zhang_multi-instance_2009} and \cite{dedic_hierarchicke_2017}.

\todo{Stochastic and information-theoretical formalisms?}
